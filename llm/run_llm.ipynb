{
 "cells": [
  {
   "cell_type": "code",
   "id": "bf7d153b-91ff-4970-8e08-590a5b648960",
   "metadata": {},
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34a51563-f077-482f-882b-5615f94cb1dd",
   "metadata": {},
   "source": [
    "llm = LLM(model=\"Qwen/Qwen2.5-Coder-32B-Instruct\", tensor_parallel_size=4, max_model_len=128000)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-32B-Instruct\", local_files_only=True,\n",
    "                                              cache_dir=\"/mnt/nobackup/models\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b6b9d671-2d6b-40a9-a48c-85fb29c07ada",
   "metadata": {},
   "source": [
    "sampling_params = SamplingParams(temperature=0.8,\n",
    "                                     top_p=0.9,\n",
    "                                     min_p=0.0,\n",
    "                                     repetition_penalty=1.2,\n",
    "                                     max_tokens=128000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a5d655c-f83a-465a-9354-c9c798ceb63b",
   "metadata": {},
   "source": [
    "def tokenizing(message, tokenizer):\n",
    "    # Apply the tokenizer to the message list directly\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        conversation=message,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return [text]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2dc10115-fd36-4a87-934c-c3674df99dad",
   "metadata": {},
   "source": [
    "def run_qwen(llm, tokenizer, input_file_path, action_to_test, output_folder):\n",
    "    # Read the input file content\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        input_shortcut = input_file.read()\n",
    "    history = None\n",
    "    # Perform the 'spy' action\n",
    "    if action_to_test == 'spy':\n",
    "        history = run_spy_stalk(llm, tokenizer, input_shortcut)\n",
    "    elif action_to_test == 'overload':\n",
    "        history = run_overload(llm, tokenizer, input_shortcut)\n",
    "    elif action_to_test == 'lockout':\n",
    "        history = run_lockout(llm, tokenizer, input_shortcut)\n",
    "    elif action_to_test == 'impersonation':\n",
    "        history = run_impersonation(llm, tokenizer, input_shortcut)\n",
    "\n",
    "    # Extract the input file name (without extension)\n",
    "    input_file_name = os.path.splitext(os.path.basename(input_file_path))[0]\n",
    "\n",
    "    # Create the subfolder for the action inside the output folder\n",
    "    action_folder = os.path.join(output_folder, action_to_test)\n",
    "    os.makedirs(action_folder, exist_ok=True)\n",
    "\n",
    "    history_file_path = os.path.join(action_folder, f\"{input_file_name}-{action_to_test}.json\")\n",
    "    with open(history_file_path, 'w', encoding='utf-8') as history_file:\n",
    "        if isinstance(history, list):\n",
    "            # Serialize the list of dictionaries to JSON\n",
    "            json.dump(history, history_file, indent=2, ensure_ascii=False)\n",
    "        else:\n",
    "            # Handle cases where history is not a list\n",
    "            json.dump([history], history_file, indent=2, ensure_ascii=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8183186b-52c3-444c-85a4-1a2ac80128dd",
   "metadata": {},
   "source": [
    "def run_spy_stalk(llm, tokenizer, input_file):\n",
    "    # First Step\n",
    "    first_step_prompt_path = 'spy_stalk_prompt/first_step_spy_stalk.txt'\n",
    "    with open(first_step_prompt_path, 'r', encoding='utf-8') as file:\n",
    "        first_step = file.read()\n",
    "    message = [\n",
    "        {\"role\": \"system\", \"content\": f\"{first_step}\\n\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{input_file}\\n\"}\n",
    "    ]\n",
    "    text_vals_first = tokenizing(message, tokenizer)\n",
    "    outputs_first = llm.generate(text_vals_first, sampling_params, use_tqdm=False)\n",
    "    for output in outputs_first:\n",
    "        generated_text = output.outputs[0].text\n",
    "        # print(generated_text)\n",
    "        if generated_text is not None and generated_text != '':\n",
    "            message.append({\"role\": \"assistant\", \"content\": f\"{generated_text}\\n\"})\n",
    "        else:\n",
    "            raise TypeError(\"no output\")\n",
    "\n",
    "    # Second Step\n",
    "    second_step_prompt_path = 'spy_stalk_prompt/second_step_spy_stalk.txt'\n",
    "    with open(second_step_prompt_path, 'r', encoding='utf-8') as file:\n",
    "        second_step = file.read()\n",
    "    message.append({\"role\": \"system\", \"content\": f\"{second_step}\\n\"})\n",
    "    second_input = tokenizing(message, tokenizer)\n",
    "    second_outputs = llm.generate(second_input, sampling_params, use_tqdm=False)\n",
    "    for output in second_outputs:\n",
    "        generated_text = output.outputs[0].text\n",
    "        # print(generated_text)\n",
    "        if generated_text is not None and generated_text != '':\n",
    "            message.append({\"role\": \"assistant\", \"content\": f\"{generated_text}\\n\"})\n",
    "        else:\n",
    "            raise TypeError(\"no output\")\n",
    "\n",
    "    # Third Step\n",
    "    third_step_prompt_path = 'spy_stalk_prompt/third_step_spy_stalk.txt'\n",
    "    with open(third_step_prompt_path, 'r', encoding='utf-8') as file:\n",
    "        third_step = file.read()\n",
    "    message.append({\"role\": \"system\", \"content\": f\"{third_step}\\n\"})\n",
    "    third_input = tokenizing(message, tokenizer)\n",
    "    outputs_third = llm.generate(third_input, sampling_params, use_tqdm=False)\n",
    "    for output in outputs_third:\n",
    "        generated_text = output.outputs[0].text\n",
    "        # print(generated_text)\n",
    "        if generated_text is not None and generated_text != '':\n",
    "            message.append({\"role\": \"assistant\", \"content\": f\"{generated_text}\\n\"})\n",
    "        else:\n",
    "            raise TypeError(\"no output\")\n",
    "    return message"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "39a8c30857674406",
   "metadata": {},
   "source": [
    "def run_overload(llm, tokenizer, input_file):\n",
    "    # First Step\n",
    "    first_step_prompt_path = 'overloading_prompt/first_step_overloading.txt'\n",
    "    with open(first_step_prompt_path, 'r', encoding='utf-8') as file:\n",
    "        first_step = file.read()\n",
    "    message = [\n",
    "        {\"role\": \"system\", \"content\": f\"{first_step}\\n\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{input_file}\\n\"}\n",
    "    ]\n",
    "    text_vals_first = tokenizing(message, tokenizer)\n",
    "    outputs_first = llm.generate(text_vals_first, sampling_params, use_tqdm=False)\n",
    "    for output in outputs_first:\n",
    "        generated_text = output.outputs[0].text\n",
    "        # print(generated_text)\n",
    "        if generated_text is not None and generated_text != '':\n",
    "            message.append({\"role\": \"assistant\", \"content\": f\"{generated_text}\\n\"})\n",
    "        else:\n",
    "            raise TypeError(\"no output\")\n",
    "\n",
    "    # Second Step\n",
    "    second_step_prompt_path = 'overloading_prompt/second_step_overloading.txt'\n",
    "    with open(second_step_prompt_path, 'r', encoding='utf-8') as file:\n",
    "        second_step = file.read()\n",
    "    message.append({\"role\": \"system\", \"content\": f\"{second_step}\\n\"})\n",
    "    second_input = tokenizing(message, tokenizer)\n",
    "    second_outputs = llm.generate(second_input, sampling_params, use_tqdm=False)\n",
    "    for output in second_outputs:\n",
    "        generated_text = output.outputs[0].text\n",
    "        # print(generated_text)\n",
    "        if generated_text is not None and generated_text != '':\n",
    "            message.append({\"role\": \"assistant\", \"content\": f\"{generated_text}\\n\"})\n",
    "        else:\n",
    "            raise TypeError(\"no output\")\n",
    "\n",
    "    return message"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "addc471a444c86f5",
   "metadata": {},
   "source": [
    "def run_lockout(llm, tokenizer, input_file):\n",
    "    # First Step\n",
    "    first_step_prompt_path = 'lockout_prompt/first_step_lock.txt'\n",
    "    with open(first_step_prompt_path, 'r', encoding='utf-8') as file:\n",
    "        first_step = file.read()\n",
    "    message = [\n",
    "        {\"role\": \"system\", \"content\": f\"{first_step}\\n\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{input_file}\\n\"}\n",
    "    ]\n",
    "    text_vals_first = tokenizing(message, tokenizer)\n",
    "    outputs_first = llm.generate(text_vals_first, sampling_params, use_tqdm=False)\n",
    "    for output in outputs_first:\n",
    "        generated_text = output.outputs[0].text\n",
    "        # print(generated_text)\n",
    "        if generated_text is not None and generated_text != '':\n",
    "            message.append({\"role\": \"assistant\", \"content\": f\"{generated_text}\\n\"})\n",
    "        else:\n",
    "            raise TypeError(\"no output\")\n",
    "\n",
    "    # Second Step\n",
    "    second_step_prompt_path = 'lockout_prompt/second_step_lock.txt'\n",
    "    with open(second_step_prompt_path, 'r', encoding='utf-8') as file:\n",
    "        second_step = file.read()\n",
    "    message.append({\"role\": \"system\", \"content\": f\"{second_step}\\n\"})\n",
    "    second_input = tokenizing(message, tokenizer)\n",
    "    second_outputs = llm.generate(second_input, sampling_params, use_tqdm=False)\n",
    "    for output in second_outputs:\n",
    "        generated_text = output.outputs[0].text\n",
    "        # print(generated_text)\n",
    "        if generated_text is not None and generated_text != '':\n",
    "            message.append({\"role\": \"assistant\", \"content\": f\"{generated_text}\\n\"})\n",
    "        else:\n",
    "            raise TypeError(\"no output\")\n",
    "\n",
    "    return message"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d2e7d70103ab249",
   "metadata": {},
   "source": [
    "def run_impersonation(llm, tokenizer, input_file):\n",
    "    # First Step\n",
    "    first_step_prompt_path = 'impersonation_prompt/first_step_impersonation.txt'\n",
    "    with open(first_step_prompt_path, 'r', encoding='utf-8') as file:\n",
    "        first_step = file.read()\n",
    "    message = [\n",
    "        {\"role\": \"system\", \"content\": f\"{first_step}\\n\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{input_file}\\n\"}\n",
    "    ]\n",
    "    text_vals_first = tokenizing(message, tokenizer)\n",
    "    outputs_first = llm.generate(text_vals_first, sampling_params, use_tqdm=False)\n",
    "    for output in outputs_first:\n",
    "        generated_text = output.outputs[0].text\n",
    "        # print(generated_text)\n",
    "        if generated_text is not None and generated_text != '':\n",
    "            message.append({\"role\": \"assistant\", \"content\": f\"{generated_text}\\n\"})\n",
    "        else:\n",
    "            raise TypeError(\"no output\")\n",
    "\n",
    "    # Second Step\n",
    "    second_step_prompt_path = 'impersonation_prompt/second_step_impersonation.txt'\n",
    "    with open(second_step_prompt_path, 'r', encoding='utf-8') as file:\n",
    "        second_step = file.read()\n",
    "    message.append({\"role\": \"system\", \"content\": f\"{second_step}\\n\"})\n",
    "    second_input = tokenizing(message, tokenizer)\n",
    "    second_outputs = llm.generate(second_input, sampling_params, use_tqdm=False)\n",
    "    for output in second_outputs:\n",
    "        generated_text = output.outputs[0].text\n",
    "        # print(generated_text)\n",
    "        if generated_text is not None and generated_text != '':\n",
    "            message.append({\"role\": \"assistant\", \"content\": f\"{generated_text}\\n\"})\n",
    "        else:\n",
    "            raise TypeError(\"no output\")\n",
    "\n",
    "    return message"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "825724f5-6792-4540-835c-734426f455dd",
   "metadata": {},
   "source": [
    "output_folder = '../measurement/result_market_test_set'\n",
    "# output_folder = '../measurement/result_test_case'\n",
    "# output_folder = '../measurement/result_main_test_set'\n",
    "# output_folder = '../measurement/result_matt_test_set'\n",
    "# output_folder = '../measurement/result_share_test_set'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "deaeee14-1911-4360-b94c-902ffdd74d21",
   "metadata": {},
   "source": [
    "input_folder = '../test/market_spy'\n",
    "# input_folder = '../test/test_case/spy_stalk'\n",
    "# input_folder = '../test/spy_to_evaluate_share'\n",
    "# input_folder = '../test/spy_to_evaluate_matt'\n",
    "\n",
    "\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith('.txt'):\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        output_action_folder = os.path.join(output_folder, 'spy')\n",
    "        output_file_path = os.path.join(output_action_folder, f\"{base_name}-spy.json\")\n",
    "        \n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f\"Skipping file: {file_name} (Output {output_file_path} already exists)\")\n",
    "            continue\n",
    "        \n",
    "        input_file_path = os.path.join(input_folder, file_name)\n",
    "        print(f\"Processing file: {input_file_path}\")\n",
    "    \n",
    "        run_qwen(llm, tokenizer, input_file_path, 'spy', output_folder)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23f29c65-f1d1-46ec-bbf8-9ee502595367",
   "metadata": {},
   "source": [
    "# input_folder = '../test/test_case/overloading'\n",
    "# input_folder = '../test/overload_to_evaluate'\n",
    "input_folder = '../test/market_overload'\n",
    "# input_folder = '../test/overload_to_evaluate_share'\n",
    "# input_folder = '../test/overload_to_evaluate_matt'\n",
    "\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith('.txt'):\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        output_action_folder = os.path.join(output_folder, 'overload')\n",
    "        output_file_path = os.path.join(output_action_folder, f\"{base_name}-overload.json\")\n",
    "        \n",
    "        # if os.path.exists(output_file_path):\n",
    "        #     print(f\"Skipping file: {file_name} (Output {output_file_path} already exists)\")\n",
    "        #     continue\n",
    "        input_file_path = os.path.join(input_folder, file_name)\n",
    "        print(f\"Processing file: {input_file_path}\")\n",
    "\n",
    "        run_qwen(llm, tokenizer, input_file_path, 'overload', output_folder)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e787f2f5d605db7a",
   "metadata": {},
   "source": [
    "# input_folder = '../test/lock_to_evaluate'\n",
    "# input_folder = '../test/lock_to_evaluate_share'\n",
    "input_folder = '../test/market_lockout'\n",
    "\n",
    "\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith('.txt'):\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        output_action_folder = os.path.join(output_folder, 'lockout')\n",
    "        output_file_path = os.path.join(output_action_folder, f\"{base_name}-lockout.json\")\n",
    "        \n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f\"Skipping file: {file_name} (Output {output_file_path} already exists)\")\n",
    "            continue\n",
    "        input_file_path = os.path.join(input_folder, file_name)\n",
    "        print(f\"Processing file: {input_file_path}\")\n",
    "\n",
    "        run_qwen(llm, tokenizer, input_file_path, 'lockout', output_folder)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "16b2b925b998d443",
   "metadata": {},
   "source": [
    "# input_folder = '../test/impersonation_to_evaluate'\n",
    "input_folder = '../test/market_impersonation'\n",
    "# input_folder = '../test/impersonation_to_evaluate_matt'\n",
    "\n",
    "\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith('.txt'):\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        output_action_folder = os.path.join(output_folder, 'impersonation')\n",
    "        output_file_path = os.path.join(output_action_folder, f\"{base_name}-impersonation.json\")\n",
    "        \n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f\"Skipping file: {file_name} (Output {output_file_path} already exists)\")\n",
    "            continue\n",
    "        input_file_path = os.path.join(input_folder, file_name)\n",
    "        print(f\"Processing file: {input_file_path}\")\n",
    "\n",
    "        run_qwen(llm, tokenizer, input_file_path, 'impersonation', output_folder)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2fbb1bbd-e482-433c-9935-6d4832d1d97a",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
